{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Region list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_list_filename = 'region_list.txt' \n",
    "region_list = []\n",
    "\n",
    "with open(region_list_filename) as f:  \n",
    "    for line in f: \n",
    "        region_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(region_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(region_list[100]) as f: \n",
    "    rigatoni = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rigatoni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look here: rigatoni is the 100th dict in the regionlist. Here we look inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rigatoni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we want to extract all the values from a dict, say rigatoni in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This doesn't work\n",
    "# for sceneTag in listas f: \n",
    "#     rigatoni = json.load(f)\n",
    "    \n",
    " \n",
    "#     if sceneTag in regions and \"d2_p0_z0\" in dict.itervalues():\n",
    "#         print (sceneTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('CAMHDA301-20160511T000000Z_optical_flow_regions-Copy1.json') as f:  \n",
    "    for sceneTag in f: \n",
    "        if f[sceneTag]=='d2_p0_z0': \n",
    "            print(sceneTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json[Object.keys(json)[0]].forEach(function(d2_p0_z0){\n",
    "    $.each(d2_p0_z0, function(key, d2_p0_z0){\n",
    "        console.log(key + \": \" + d2_p0_z0);\n",
    "    });\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type (f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Learn how to use dictionary comprehensions, find a list of keys with a specific value, how to loop through the dictionary for a specific key and value. Extract the keys and values like start and end frame the corespond to the specific location value, make a list. \n",
    "put this into a panda dataframe, which will be our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_list_filename = 'region_list.txt'\n",
    "type(region_list_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = []\n",
    "scene_Tag = []\n",
    "start_Frame = []\n",
    "end_Frame = []\n",
    "region_list_filename = 'region_list.txt'\n",
    "\n",
    "with open(region_list_filename) as f:  \n",
    "    for line in f: \n",
    "        with open(line.strip()) as l:\n",
    "            data = json.load(l)\n",
    "            for region in data ['regions']:\n",
    "                if region ['type'] == 'static':\n",
    "                    try:\n",
    "                        if '_p2_z0' in region['sceneTag']:\n",
    "                            \n",
    "                            url.append(data['movie']['URL'])\n",
    "                            scene_tag.append(region['sceneTag'])\n",
    "                            start_frame.append(int(region['startFrame']))\n",
    "                            end_frame.append(int(region['endFrame']))\n",
    "                        except:\n",
    "                            continue"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reading multiple files to create a single DataFrame\n",
    "The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat():\n",
    "\n",
    "In [159]: for i in range(3):\n",
    "   .....:     data = pd.DataFrame(np.random.randn(10, 4))\n",
    "   .....:     data.to_csv('file_{}.csv'.format(i))\n",
    "   .....: \n",
    "\n",
    "In [160]: files = ['file_0.csv', 'file_1.csv', 'file_2.csv']\n",
    "\n",
    "In [161]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can use the same approach to read all files matching a pattern. Here is an example using glob:\n",
    "In [162]: import glob\n",
    "\n",
    "In [163]: files = glob.glob('file_*.csv')\n",
    "\n",
    "In [164]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parsing date components in multi-columns\n",
    "Parsing date components in multi-columns is faster with a format\n",
    "In [30]: i = pd.date_range('20000101',periods=10000)\n",
    "\n",
    "In [31]: df = pd.DataFrame(dict(year = i.year, month = i.month, day = i.day))\n",
    "\n",
    "In [32]: df.head()\n",
    "Out[32]:\n",
    "   day  month  year\n",
    "0    1      1  2000\n",
    "1    2      1  2000\n",
    "2    3      1  2000\n",
    "3    4      1  2000\n",
    "4    5      1  2000\n",
    "\n",
    "In [33]: %timeit pd.to_datetime(df.year*10000+df.month*100+df.day,format='%Y%m%d')\n",
    "100 loops, best of 3: 7.08 ms per loop\n",
    "\n",
    "# simulate combinging into a string, then parsing\n",
    "In [34]: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x['year'],x['month'],x['day']),axis=1)\n",
    "\n",
    "In [35]: ds.head()\n",
    "Out[35]:\n",
    "0    20000101\n",
    "1    20000102\n",
    "2    20000103\n",
    "3    20000104\n",
    "4    20000105\n",
    "dtype: object\n",
    "\n",
    "In [36]: %timeit pd.to_datetime(ds)\n",
    "1 loops, best of 3: 488 ms per loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
