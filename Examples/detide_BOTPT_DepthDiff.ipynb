{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: The relationship between post- 2015 eruption deformation and seismicity rates since the 2015 eruption at Axial Seamount using OOI data\n",
    "\n",
    "# Authors: J. Natalie, D. C. Soule, T. J. Crone, W. W. Chadwick Jr., W. S. D. Wilcock\n",
    "\n",
    "#### BOTPT Data detide DepthDifference\n",
    "\n",
    "#### This method is detailed here: https://www.pmel.noaa.gov/eoi/rsn/diffs.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "#### The Ocean Observatories Initiative Cabled Array (OOI-CA) provides the opportunity to use new approaches to observe geological  processes  at Axial Seamount, an active submarine volcano on the Juan de Fuca spreading ridge located ~300 miles off the coast of Oregon. The OOI-CA provides a suite of interdisciplinary real-time datasets from seismic and geodetic networks and a variety of instruments in two hydrothermal fields with which we can examine system-level processes governing the volcano-marine environment. The geophysical networks comprise 4 Pressure/Tilt (BOTPT) instruments that detect the rise, fall, and tilt of the seafloor as it responds to the movement of magma within the crust and 7 seismometers to monitor earthquakes. In order to better understand the patterns of magma storage and delivery beneath the Axial summit caldera and provide constraints for the development of improved predictive models, we are examining short-term variations in the rates of deformation and seismicity and how well they co-vary during the inter-eruption time period. We will also investigate apparent “pauses” in the magma supply rate when the rate of seismicity and deformation have both dropped. On the time-scale of years, seafloor deformation measurements show a relatively steady rate of inflation of the seafloor between eruptions that can be interpreted as reflecting the rate that magma is supplied to the sub-caldera magma reservoir (Nooner and Chadwick 2016). However, on the time-scale of weeks to months, the rate is more variable. Similarly, the rate of earthquakes detected by the OOI seismometer network also varies with time. It was very low immediately after the 2015 eruption, but it has increased markedly over the past year, and based on the observations of prior eruptive cycles, it is expected to increase by several orders of magnitude before the next eruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load map of Axial caldera showing locations of BOTPT instruments (red circles)\n",
    "from IPython.display import Image\n",
    "Image(url = \"https://www.pmel.noaa.gov/eoi/rsn/Axial-2017-OOI-caldera-ed-sm.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zoomed in map of Axial caldera\n",
    "from IPython.display import Image\n",
    "Image(url = \"https://www.pmel.noaa.gov/eoi/rsn/Axial-2017-OOI-zoom-ed-sm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Information Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = 'OOIAPI-J97520T1AYPUNI'\n",
    "TOKEN =  'TEMP-TOKEN-YD01XSNDIO57MP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "from matplotlib import pyplot\n",
    "from pylab import rcParams\n",
    "import pickle as pk\n",
    "import gc\n",
    "import requests \n",
    "import matplotlib.gridspec as gridspec\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For mad help with Pandas https://chrisalbon.com\n",
    "#### For all sorts of great repos https://github.com/rabernat/research_computing.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Downsampled BOTPT Data - I have made pickle files downsampleing the data to 15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/data/botpt/bottom_pressure15S_F11.pkl', 'rb') as E:\n",
    "    botpt_data = pk.load(E)\n",
    "df_botpt = pd.DataFrame(botpt_data)\n",
    "df_botpt['bottom_pressure'] = df_botpt['bottom_pressure'].astype(float)\n",
    "df_botpt['depth']=df_botpt['bottom_pressure'].astype(float) * 0.670\n",
    "#MJ03F_cal_depths = [MJ03F_pressure * 0.0670 for MJ03F_pressure in MJ03F_pressure]\n",
    "#list comprehention\n",
    "epoch= [i.timestamp() for i in df_botpt.index.to_pydatetime()]\n",
    "df_botpt['epoch'] = epoch\n",
    "df_botpt= df_botpt.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/data/botpt/bottom_pressure15S_E11.pkl', 'rb') as E:\n",
    "    botpt_data = pk.load(E)\n",
    "df_botptE = pd.DataFrame(botpt_data)\n",
    "df_botptE['bottom_pressure'] = df_botptE['bottom_pressure'].astype(float)\n",
    "df_botptE['depth']=df_botptE['bottom_pressure'].astype(float) * 0.670\n",
    "#MJ03F_cal_depths = [MJ03F_pressure * 0.0670 for MJ03F_pressure in MJ03F_pressure]\n",
    "#list comprehention\n",
    "epoch= [i.timestamp() for i in df_botptE.index.to_pydatetime()]\n",
    "df_botptE['epoch'] = epoch\n",
    "df_botptE= df_botptE.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic Commands \n",
    "# len(df_botpt)\n",
    "# len(df_botptE)\n",
    "# df_botpt.tail()\n",
    "# df_botptE.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_botptE['epoch']\n",
    "del df_botptE['bottom_pressure']\n",
    "del df_botpt['epoch']\n",
    "del df_botpt['bottom_pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df_botptE.head()\n",
    "# y = df_botpt.head()\n",
    "# z = pd.merge(x,y,how='outer', indicator=True, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge BOTPT E and BOTPT F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(df_botpt, df_botptE,how='outer', indicator=True, left_index=True, right_index=True, suffixes=('_F', '_E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_botptMerge = test[test['_merge'] == 'both']\n",
    "del df_botptMerge['_merge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_botptMerge = df_botptMerge.loc['2017-01-1 00:00:00':'2018-10-31 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_botpt[df_botpt['_merge'] == 'right_only']\n",
    "\n",
    "# df_botpt[df_botpt['_merge'] == 'left_only']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Depth difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthDiff = df_botptMerge['depth_E'].values - df_botptMerge['depth_F'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_botptMerge['diff'] = depthDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 = df_botptMerge['depth_F'].tolist()\n",
    "# h1 = df_botptMerge['depth_E'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def subtract_lists(h1, h2):\n",
    "#     diff = [a - b for a,b in zip(h1, h2)]\n",
    "#     return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height = subtract_lists(h1,h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create time and height vectors for plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = list(df_botptMerge.index.values)\n",
    "#height = x.tolist()\n",
    "height = df_botptMerge['diff'].tolist()\n",
    "time_int = []\n",
    "time = list(pd.to_datetime(df_botptMerge.index.values))\n",
    "for i in time:\n",
    "    i = np.datetime64(i).astype(datetime.datetime)\n",
    "    time_int.append(dates.date2num(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(28, 6)\n",
    "hb1 = ax.hexbin(time_int, height, vmin=0, vmax=30, gridsize=(1500,100), mincnt=1, cmap='Greens', linewidths=0)\n",
    "fig.colorbar(hb1, pad = 0.01)\n",
    "ax.yaxis.grid(True)\n",
    "ax.xaxis.grid(True)\n",
    "ax.set_xlim(datetime.datetime(2017, 1, 1, 0, 0),datetime.datetime(2018, 10, 31, 0, 0))\n",
    "years = dates.YearLocator()\n",
    "months = dates.MonthLocator()\n",
    "yearsFmt = dates.DateFormatter('\\n\\n\\n%Y')\n",
    "monthsFmt = dates.DateFormatter('%b')\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(monthsFmt)\n",
    "ax.xaxis.set_minor_locator(years)\n",
    "ax.xaxis.set_minor_formatter(yearsFmt)\n",
    "plt.tight_layout()\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This method appears to do a pretty good of eliminating the tidal signal. You can clearly see the periods of deflation near 9/2016, 2/2017, 1/2018, and 7/2018\n",
    "\n",
    "##### There is more work to do to eliminate the dropouts and calculate the differing rates of inflation or deflation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('Figures/2015thruPresent.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_botptMerge['date']=pd.DatetimeIndex(df_botptMerge.index).date\n",
    "#df_botptMerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Groupby to create one day mean measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_botptMean=df_botptMerge.groupby('date').mean()\n",
    "df_botptMean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create time and height vectors for plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = list(df_botptMerge.index.values)\n",
    "#height = x.tolist()\n",
    "height = df_botptMean['diff'].tolist()\n",
    "time_int = []\n",
    "time = list(pd.to_datetime(df_botptMean.index.values))\n",
    "for i in time:\n",
    "    i = np.datetime64(i).astype(datetime.datetime)\n",
    "    time_int.append(dates.date2num(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot One Day Measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(28, 6)\n",
    "hb1 = ax.plot(time_int, height)\n",
    "ax.yaxis.grid(True)\n",
    "ax.xaxis.grid(True)\n",
    "ax.set_xlim(datetime.datetime(2017, 1, 1, 0, 0),datetime.datetime(2018, 10, 31, 0, 0))\n",
    "years = dates.YearLocator()\n",
    "months = dates.MonthLocator()\n",
    "yearsFmt = dates.DateFormatter('\\n\\n\\n%Y')\n",
    "monthsFmt = dates.DateFormatter('%b')\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(monthsFmt)\n",
    "ax.xaxis.set_minor_locator(years)\n",
    "ax.xaxis.set_minor_formatter(yearsFmt)\n",
    "plt.tight_layout()\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haley Cabaniss with significant help from Bill Chadwick, Zach Billings, Sage Lichtenwainer, and Liana Vaccari\n",
    "\n",
    "### Bill Chadwick recently noticed spikes in BOTPT data for 3 of the instruments on the cabled array. He notified Dana Manalung, who attributed the spikes to a grounding problem in the junction boxes. During a recent cruise (early July 2018) the junction boxes were changed and the 3 BOTPT instruments (MJ03B, MJ03B, MJ03F) were grounded using an alternatitve method. \n",
    "\n",
    "### Remove Spikes \n",
    "These drop outs come from slight mismatches between the stations. To remove them I have identified the spikes where the data indicates an unreasonable change in seafloor height and replaced them with an adjacent data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_data = np.array(height)\n",
    "time_array = np.array(time)\n",
    "\n",
    "cleaner_data = []\n",
    "cleaner_time = []\n",
    "\n",
    "# i = 0\n",
    "# while i < len(weird_data):\n",
    "#     if (weird_data[i] - weird_data[i-1] > 2\n",
    "\n",
    "length = len(weird_data)\n",
    "\n",
    "i = 0\n",
    "while i < length:\n",
    "    j = i+2\n",
    "    while j < length:\n",
    "        if np.abs(weird_data[i] - weird_data[j]) < .05:\n",
    "            cleaner_data.append(weird_data[j])\n",
    "            cleaner_time.append(time_array[j])\n",
    "            break\n",
    "        else:\n",
    "            j += 1\n",
    "    i = j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between plots with and without spikes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig3, (ax1,ax2) = plt.subplots(2,1)\n",
    "fig3.set_size_inches(28, 14)\n",
    "hb1 = ax1.plot(time_int, height, linewidth=5)\n",
    "ax1.yaxis.grid(True)\n",
    "ax1.xaxis.grid(True)\n",
    "ax1.set_xlim(datetime.datetime(2017, 1, 1, 0, 0),datetime.datetime(2018, 10, 31, 0, 0))\n",
    "years = dates.YearLocator()\n",
    "months = dates.MonthLocator()\n",
    "yearsFmt = dates.DateFormatter('\\n\\n\\n%Y')\n",
    "monthsFmt = dates.DateFormatter('%b')\n",
    "ax1.xaxis.set_major_locator(months)\n",
    "ax1.xaxis.set_major_formatter(monthsFmt)\n",
    "ax1.xaxis.set_minor_locator(years)\n",
    "ax1.xaxis.set_minor_formatter(yearsFmt)\n",
    "\n",
    "hb1 = ax2.plot(cleaner_time, cleaner_data, linewidth=5)\n",
    "ax2.yaxis.grid(True)\n",
    "ax2.xaxis.grid(True)\n",
    "ax2.set_xlim(datetime.datetime(2017, 1, 1, 0, 0),datetime.datetime(2018, 10, 31, 0, 0))\n",
    "years = dates.YearLocator()\n",
    "months = dates.MonthLocator()\n",
    "yearsFmt = dates.DateFormatter('\\n\\n\\n%Y')\n",
    "monthsFmt = dates.DateFormatter('%b')\n",
    "ax2.xaxis.set_major_locator(months)\n",
    "ax2.xaxis.set_major_formatter(monthsFmt)\n",
    "ax2.xaxis.set_minor_locator(years)\n",
    "ax2.xaxis.set_minor_formatter(yearsFmt)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n",
    "plt.suptitle('Removing Outliers', fontsize=32, color= 'red', fontweight = 'bold')\n",
    "plt.subplots_adjust(top=0.90)\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-dataframe\n",
    "\n",
    "https://stackoverflow.com/questions/35829364/fixing-inflexion-point-estimate-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('Figures/2018.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Seismic Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic_file = '/home/jovyan/data/hypo71_2018.dat'\n",
    "df_seismic_data = pd.read_csv(seismic_file, delim_whitespace=True, dtype=object)\n",
    "df_seismic_data['datetime'] = df_seismic_data['yyyymmdd'] + 'T' + \\\n",
    "            df_seismic_data['HHMM'].str.slice(start=0, stop=2) + ':' + \\\n",
    "            df_seismic_data['HHMM'].str.slice(start=2) \n",
    "df_seismic_data.index = pd.to_datetime(df_seismic_data['datetime'].values)\n",
    "df_seismic_data['datetime'] = pd.to_datetime(df_seismic_data['datetime'].values)\n",
    "df_seismic_data = df_seismic_data.loc['2017-01-1 00:00:00':'2018-10-31 00:00:00']\n",
    "del df_seismic_data['yyyymmdd']\n",
    "del df_seismic_data['HHMM']\n",
    "del df_seismic_data['SSS.SS']\n",
    "del df_seismic_data['MW']\n",
    "del df_seismic_data['NWR']\n",
    "del df_seismic_data['GAP']\n",
    "del df_seismic_data['DMIN']\n",
    "del df_seismic_data['ERH']\n",
    "del df_seismic_data['ERZ']\n",
    "del df_seismic_data['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seismic_data.datetime.astype(np.int64).values/1e64\n",
    "df_seismic_data['date'] =pd.DatetimeIndex(df_seismic_data.datetime).date\n",
    "#df_seismic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe with Earthquake frequency (count per day)\n",
    "##### Note: Days with zero earthquakes are not represented in this timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eqMean=df_seismic_data.groupby('date').count()\n",
    "del df_eqMean['Lat(D']\n",
    "del df_eqMean['M)']\n",
    "del df_eqMean['Lon(D']\n",
    "del df_eqMean['M).1']\n",
    "del df_eqMean['Depth']\n",
    "del df_eqMean['RMS']\n",
    "del df_eqMean['PMom']\n",
    "del df_eqMean['SMom']\n",
    "df_eqMean['count']= df_eqMean.datetime\n",
    "del df_eqMean['datetime']\n",
    "df_eqMean.columns.name = df_eqMean.index.name\n",
    "df_eqMean.index.name = None\n",
    "#df_eqMean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample. Add days to our earthquake data where there were zero earthquakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range('2017-01-1 00:00:00', '2018-10-31 00:00:00')\n",
    "\n",
    "s = df_eqMean\n",
    "\n",
    "s.index = pd.DatetimeIndex(s.index)\n",
    "\n",
    "s = s.reindex(idx, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create time and count vectors earthquake frequency vectors for plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_eqMean['count'].tolist()\n",
    "time_int = []\n",
    "time = list(pd.to_datetime(df_eqMean.index.values))\n",
    "for i in time:\n",
    "    i = np.datetime64(i).astype(datetime.datetime)\n",
    "    time_int.append(dates.date2num(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingaverage(interval, window_size):\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth data using rolling window that chops off 95th percentile \n",
    "count_av = list(movingaverage(df_eqMean['count'],7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of Earthquake Frequency and Inflation Rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig4, (ax1,ax2) = plt.subplots(2,1)\n",
    "fig4.set_size_inches(28, 14)\n",
    "hb1 = ax1.plot(cleaner_time, cleaner_data, linewidth=5)\n",
    "ax1.yaxis.grid(True)\n",
    "ax1.xaxis.grid(True)\n",
    "ax1.set_xlim(datetime.datetime(2017, 1, 1, 0, 0),datetime.datetime(2018, 10, 31, 0, 0))\n",
    "years = dates.YearLocator()\n",
    "months = dates.MonthLocator()\n",
    "yearsFmt = dates.DateFormatter('\\n\\n\\n%Y')\n",
    "monthsFmt = dates.DateFormatter('%b')\n",
    "ax1.xaxis.set_major_locator(months)\n",
    "ax1.xaxis.set_major_formatter(monthsFmt)\n",
    "ax1.xaxis.set_minor_locator(years)\n",
    "ax1.xaxis.set_minor_formatter(yearsFmt)\n",
    "ax1.set_title('Caldera Inflation', fontsize=18, fontweight = 'bold')\n",
    "\n",
    "\n",
    "hb1 = ax2.plot(time, count)\n",
    "hb2 = ax2.plot(time, count_av, linewidth=5)\n",
    "ax2.yaxis.grid(True)\n",
    "ax2.xaxis.grid(True)\n",
    "ax2.set_xlim(datetime.datetime(2017, 1, 1, 0, 0),datetime.datetime(2018, 10, 31, 0, 0))\n",
    "years = dates.YearLocator()\n",
    "months = dates.MonthLocator()\n",
    "yearsFmt = dates.DateFormatter('\\n\\n\\n%Y')\n",
    "monthsFmt = dates.DateFormatter('%b')\n",
    "ax2.xaxis.set_major_locator(months)\n",
    "ax2.xaxis.set_major_formatter(monthsFmt)\n",
    "ax2.xaxis.set_minor_locator(years)\n",
    "ax2.xaxis.set_minor_formatter(yearsFmt)\n",
    "ax2.set_title('Daily Seismicity', fontsize=18, fontweight = 'bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n",
    "plt.suptitle('Comparison Between Inflation and Seismicity', fontsize=32, color= 'blue', fontweight = 'bold')\n",
    "plt.subplots_adjust(top=0.90)\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "#### In order to better understand the patterns of magma storage and delivery beneath the Axial summit caldera and provide constraints for the development of improved predictive models, we are examining short-term variations in the rates of deformation and seismicity and how well they co-vary during the inter-eruption time period. We will also investigate apparent “pauses” in the magma supply rate when the rate of seismicity and deformation have both dropped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Playground: Everything beneath this point is code and analysis I am experimenting with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify inflection points for inflation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turningpoints(cleaner_data):\n",
    "    dx = np.diff(cleaner_data)\n",
    "    return np.sum(dx[1:] * dx[:-1] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turning_points(array):\n",
    "    ''' turning_points(array) -> min_indices, max_indices\n",
    "    Finds the turning points within an 1D array and returns the indices of the minimum and \n",
    "    maximum turning points in two separate lists.\n",
    "    '''\n",
    "    idx_max, idx_min = [], []\n",
    "    if (len(array) < 3): \n",
    "        return idx_min, idx_max\n",
    "\n",
    "    NEUTRAL, RISING, FALLING = range(3)\n",
    "    def get_state(a, b):\n",
    "        if a < b: return RISING\n",
    "        if a > b: return FALLING\n",
    "        return NEUTRAL\n",
    "\n",
    "    ps = get_state(array[0], array[1])\n",
    "    begin = 1\n",
    "    for i in range(2, len(array)):\n",
    "        s = get_state(array[i - 1], array[i])\n",
    "        if s != NEUTRAL:\n",
    "            if ps != NEUTRAL and ps != s:\n",
    "                if s == FALLING: \n",
    "                    idx_max.append((begin + i - 1) // 2)\n",
    "                else:\n",
    "                    idx_min.append((begin + i - 1) // 2)\n",
    "            begin = i\n",
    "            ps = s\n",
    "    return idx_min, idx_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(len(x) for x in turning_points(cleaner_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Cross-Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "sig = np.repeat([0., 1., 1., 0., 1., 0., 0., 1.], 128)\n",
    "sig_noise = sig + np.random.randn(len(sig))\n",
    "corr = signal.correlate(sig_noise, np.ones(128), mode='same') / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "clock = np.arange(64, len(sig), 128)\n",
    "fig, (ax_orig, ax_noise, ax_corr) = plt.subplots(3, 1, sharex=True)\n",
    "ax_orig.plot(sig)\n",
    "ax_orig.plot(clock, sig[clock], 'ro')\n",
    "ax_orig.set_title('Original signal')\n",
    "ax_noise.plot(sig_noise)\n",
    "ax_noise.set_title('Signal with noise')\n",
    "ax_corr.plot(corr)\n",
    "ax_corr.plot(clock, corr[clock], 'ro')\n",
    "ax_corr.axhline(0.5, ls=':')\n",
    "ax_corr.set_title('Cross-correlated with rectangular pulse')\n",
    "ax_orig.margins(0, 0.1)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Correlate Smoothed Seismic with Smoothed Inflation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
